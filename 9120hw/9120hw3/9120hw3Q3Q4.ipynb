{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ba35b8-4dae-4068-90dc-755783d7799b",
   "metadata": {},
   "source": [
    "## Question 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc7922-ed36-4499-b309-adf19e5af6ec",
   "metadata": {},
   "source": [
    "Pick pick 10 images of animals (such as dogs, cats, birds, farm animals, etc.). If the subject does not occupy a reasonable part of the image, then crop the image. \n",
    "\n",
    "Now use a pretrained image classification CNN as in Lab 10.9.4 to predict the class of each of your images, and report the probabilities for the top five predicted classes for each image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fae468-14ea-4b27-b2af-2e909f86f819",
   "metadata": {},
   "source": [
    "We now read in the images and preprocess them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1ddf75cc-261f-4a88-8b19-dca6b2bbf09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 224, 224])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize, CenterCrop, Normalize, Compose\n",
    "from glob import glob\n",
    "\n",
    "resize = Resize((232,232))\n",
    "crop = CenterCrop (224)\n",
    "normalize = Normalize([0.485,0.456,0.406],\n",
    "                      [0.229 ,0.224 ,0.225])\n",
    "imgfiles = sorted([f for f in glob('images/*')])\n",
    "imgs = torch.stack([torch.div(crop(resize(read_image(f))), 255) \n",
    "                    for f in imgfiles])\n",
    "imgs = normalize(imgs) \n",
    "imgs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc6e34-d872-4d31-a4fd-036f6af4cb58",
   "metadata": {},
   "source": [
    "We now set up the trained network with the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "43289d71-e9de-46d5-a428-0130d854be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchsummary import summary\n",
    "\n",
    "resnet_model = resnet50(weights=ResNet50_Weights.DEFAULT) \n",
    "# summary(resnet_model,input_data=imgs, col_names=['input_size', 'output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af896110-0090-4ec0-bf21-8a89f26feefc",
   "metadata": {},
   "source": [
    "We set the mode to eval() to ensure that the model is ready to predict on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6bd91231-e274-4c94-bf53-d265e96ad291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f335206-7648-44c8-99ee-f627d8be854a",
   "metadata": {},
   "source": [
    "We now feed our 10 images through the fitted network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "48b0e718-3229-45c6-b210-bf8b4bf519d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preds = resnet_model(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57764f-35cc-43a0-8d25-a1fcc65f9e96",
   "metadata": {},
   "source": [
    "Let’s look at the predicted probabilities for each of the top 5 choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f8d5b6c4-79c2-4deb-a724-3970f4b241ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_probs = np.exp(np.asarray(img_preds.detach())) \n",
    "img_probs /= img_probs.sum(1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95bde0-afc5-4747-8b3f-a0b4812250b3",
   "metadata": {},
   "source": [
    "In order to see the class labels, we must download the index file associated with imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "44bafd0d-55e7-429a-92b2-32e62d9ae43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "labs = json.load(open('imagenet_class_index.json')) \n",
    "class_labels = pd.DataFrame([(int(k), v[1]) for k, v in\n",
    "labs.items()],\n",
    "columns=['idx', 'label']) \n",
    "class_labels = class_labels.set_index('idx')\n",
    "class_labels = class_labels.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d94a6-2a29-4dd8-97da-230336924d30",
   "metadata": {},
   "source": [
    "We’ll now construct a data frame for each image file with the labels with the 5 highest probabilities as estimated by the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c0276572-5cfe-41a2-9e4f-67ffa0b613fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: images/ squirrel.jpg\n",
      "                    label      prob\n",
      "0            fox_squirrel  0.497437\n",
      "1  red-breasted_merganser  0.005409\n",
      "2                  marmot  0.004642\n",
      "3                    corn  0.002736\n",
      "4                     ear  0.002391\n",
      "Image: images/bluebird.jpg\n",
      "            label      prob\n",
      "0            kite  0.453833\n",
      "1  great_grey_owl  0.015914\n",
      "2             jay  0.012210\n",
      "3           quail  0.008303\n",
      "4           macaw  0.005181\n",
      "Image: images/cat.jpg\n",
      "         label      prob\n",
      "0  Persian_cat  0.163070\n",
      "1        tabby  0.074143\n",
      "2    tiger_cat  0.042578\n",
      "3      doormat  0.034508\n",
      "4  paper_towel  0.015525\n",
      "Image: images/dog.jpg\n",
      "             label      prob\n",
      "0            Lhasa  0.260317\n",
      "1         Shih-Tzu  0.097196\n",
      "2  Tibetan_terrier  0.032820\n",
      "3   cocker_spaniel  0.005889\n",
      "4         Pekinese  0.005229\n",
      "Image: images/flamingo.jpg\n",
      "            label      prob\n",
      "0        flamingo  0.609515\n",
      "1       spoonbill  0.013586\n",
      "2  American_egret  0.002132\n",
      "3         pelican  0.001365\n",
      "4           crane  0.001264\n",
      "Image: images/fox.jpg\n",
      "      label      prob\n",
      "0   kit_fox  0.610006\n",
      "1  grey_fox  0.080147\n",
      "2   red_fox  0.041770\n",
      "3     dhole  0.024135\n",
      "4  red_wolf  0.008149\n",
      "Image: images/hozen.jpg\n",
      "           label      prob\n",
      "0           lynx  0.504153\n",
      "1   snow_leopard  0.008388\n",
      "2        leopard  0.005998\n",
      "3   Egyptian_cat  0.003663\n",
      "4  ruffed_grouse  0.003290\n",
      "Image: images/parrot.jpg\n",
      "                      label      prob\n",
      "0                     macaw  0.526116\n",
      "1              African_grey  0.002219\n",
      "2  sulphur-crested_cockatoo  0.002151\n",
      "3                  lorikeet  0.001786\n",
      "4               feather_boa  0.000978\n",
      "Image: images/redpanda.jpg\n",
      "          label      prob\n",
      "0  lesser_panda  0.531708\n",
      "1       polecat  0.005819\n",
      "2   giant_panda  0.001707\n",
      "3          mink  0.001697\n",
      "4         patas  0.001639\n",
      "Image: images/yellowbird.jpg\n",
      "       label      prob\n",
      "0    jacamar  0.297499\n",
      "1      macaw  0.068107\n",
      "2   lorikeet  0.051105\n",
      "3  bee_eater  0.044430\n",
      "4     bulbul  0.029123\n"
     ]
    }
   ],
   "source": [
    "for i, imgfile in enumerate(imgfiles):\n",
    "    img_df = class_labels.copy()\n",
    "    img_df['prob'] = img_probs[i]\n",
    "    img_df = img_df.sort_values(by='prob', ascending=False)[:5] \n",
    "    print(f'Image: {imgfile}') \n",
    "    print(img_df.reset_index().drop(columns=['idx']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99377a1b-07bd-4e75-b1d1-74230b8b2831",
   "metadata": {},
   "source": [
    "## Question 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3762298-6898-49e7-9487-2f3ff47d747e",
   "metadata": {},
   "source": [
    "Repeat the analysis of Lab 10.9.5 on the IMDb data using a similarlystructured neural network. We used 16 hidden units at each of twohidden layers. Explore the effect of increasing this to 32 and 64 unitsper layer, with and without 30% dropout regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b6362918-25f9-4562-b4a6-f81de7db681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ISLP.torch import (SimpleDataModule,\n",
    "                        SimpleModule,\n",
    "                        ErrorTracker,\n",
    "                        rec_num_workers)\n",
    "from ISLP.torch.imdb import (load_lookup,\n",
    "                             load_tensor,\n",
    "                             load_sparse,\n",
    "                             load_sequential)\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning import Trainer\n",
    "from ISLP.torch.imdb import _get_imdb\n",
    "from torchinfo import summary\n",
    "from torch.optim import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "accfb4a3-7cce-466d-9077-a26460f31f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/ISLP/torch/imdb.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  S_test) = [torch.load(_get_imdb(f'IMDB_{r}', root))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   1,   14,   22,   16,   43,  530,  973, 1622, 1385,   65,  458,\n",
       "       4468], dtype=int32)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(imdb_seq_train, \n",
    " imdb_seq_test) = load_sequential(root='data/IMDB')\n",
    "padded_sample = np.asarray(imdb_seq_train.tensors[0][0])\n",
    "sample_review = padded_sample[padded_sample > 0][:12]\n",
    "sample_review[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a32cbd07-6e0b-4666-9073-ab7fa553b3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's\""
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup = load_lookup(root='data/IMDB')\n",
    "' '.join(lookup[i] for i in sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ece26-689d-4f2d-b37b-0bba09368e84",
   "metadata": {},
   "source": [
    "For our first model, we have created a binary feature for each of the 10,000 possible words in the dataset, with an entry of one in the i, j entry if word j appears in review i. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e0c70bc1-549b-42cd-8c7d-e3b45e2d066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "((X_train, Y_train),(X_valid, Y_valid),(X_test, Y_test)) = load_sparse(\n",
    "     validation=2000,\n",
    "    random_state=0,\n",
    "    root='data/IMDB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c8c15-a36d-428a-9ca7-8ea95f00181c",
   "metadata": {},
   "source": [
    "We’ll use a two-layer model for our first model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130dffb8-144b-42b6-a028-ecb7a33d7456",
   "metadata": {},
   "source": [
    "### 32 units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a3d85e13-9166-49a3-ae25-4ab5fd41ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBModel(nn.Module):\n",
    "    def __init__(self, input_size): \n",
    "        super(IMDBModel, self).__init__() \n",
    "        self.dense1 = nn.Linear(input_size, 32) \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(32, 32) \n",
    "        self.output = nn.Linear(32, 1)\n",
    "    def forward(self, x):\n",
    "        val = x\n",
    "        for _map in [self.dense1,\n",
    "                     self.activation , \n",
    "                     self.dense2, \n",
    "                     self.activation , \n",
    "                     self.output]:\n",
    "            val = _map(val) \n",
    "            return torch.flatten(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9cf8f7-0e5e-4934-a7c7-577aed160f9f",
   "metadata": {},
   "source": [
    "We now instantiate our model and look at a summary (not shown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "979d00a1-9a2e-415e-8ce9-6c1b46031373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "IMDBModel                                [25000, 10003]            [800000]                  1,089\n",
       "├─Linear: 1-1                            [25000, 10003]            [25000, 32]               320,128\n",
       "===================================================================================================================\n",
       "Total params: 321,217\n",
       "Trainable params: 321,217\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 8.00\n",
       "===================================================================================================================\n",
       "Input size (MB): 1000.30\n",
       "Forward/backward pass size (MB): 6.40\n",
       "Params size (MB): 1.28\n",
       "Estimated Total Size (MB): 1007.98\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_model = IMDBModel(imdb_test.tensors[0].size()[1]) \n",
    "summary(imdb_model,\n",
    "        input_size=imdb_test.tensors[0].size(), \n",
    "        col_names=['input_size','output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be934c1-d4d3-4675-ba6f-9fec793f97f5",
   "metadata": {},
   "source": [
    "### 32 units per layer with 0.3 dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "da432450-6184-492b-90c6-616b4324e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBModel(nn.Module):\n",
    "    def __init__(self, input_size): \n",
    "        super(IMDBModel, self).__init__() \n",
    "        self.dense1 = nn.Linear(input_size, 32) \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(32, 32) \n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        val = x\n",
    "        for _map in [self.dense1,\n",
    "                     self.activation , \n",
    "                     self.dense2, \n",
    "                     self.activation , \n",
    "                     self.output,\n",
    "                     self.dropout]:\n",
    "            val = _map(val) \n",
    "            return torch.flatten(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "04d70489-82d7-4c98-be58-ddf4c0795e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "IMDBModel                                [25000, 10003]            [800000]                  1,089\n",
       "├─Linear: 1-1                            [25000, 10003]            [25000, 32]               320,128\n",
       "===================================================================================================================\n",
       "Total params: 321,217\n",
       "Trainable params: 321,217\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 8.00\n",
       "===================================================================================================================\n",
       "Input size (MB): 1000.30\n",
       "Forward/backward pass size (MB): 6.40\n",
       "Params size (MB): 1.28\n",
       "Estimated Total Size (MB): 1007.98\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_model = IMDBModel(imdb_test.tensors[0].size()[1]) \n",
    "summary(imdb_model,\n",
    "        input_size=imdb_test.tensors[0].size(), \n",
    "        col_names=['input_size','output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558fb09-925d-4e06-b1cc-e9fc14609e6c",
   "metadata": {},
   "source": [
    "### 64 units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "08283aac-d4e1-4169-8b58-55f7b647315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBModel(nn.Module):\n",
    "    def __init__(self, input_size): \n",
    "        super(IMDBModel, self).__init__() \n",
    "        self.dense1 = nn.Linear(input_size, 64) \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(64, 64) \n",
    "        self.output = nn.Linear(64, 1)\n",
    "    def forward(self, x):\n",
    "        val = x\n",
    "        for _map in [self.dense1,\n",
    "                     self.activation , \n",
    "                     self.dense2, \n",
    "                     self.activation , \n",
    "                     self.output]:\n",
    "            val = _map(val) \n",
    "            return torch.flatten(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5bd10fa9-b662-4f26-82ef-50ed7b705151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "IMDBModel                                [25000, 10003]            [1600000]                 4,225\n",
       "├─Linear: 1-1                            [25000, 10003]            [25000, 64]               640,256\n",
       "===================================================================================================================\n",
       "Total params: 644,481\n",
       "Trainable params: 644,481\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 16.01\n",
       "===================================================================================================================\n",
       "Input size (MB): 1000.30\n",
       "Forward/backward pass size (MB): 12.80\n",
       "Params size (MB): 2.56\n",
       "Estimated Total Size (MB): 1015.66\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_model = IMDBModel(imdb_test.tensors[0].size()[1]) \n",
    "summary(imdb_model,\n",
    "        input_size=imdb_test.tensors[0].size(), \n",
    "        col_names=['input_size','output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce566943-8096-487c-b38d-e91057991ab9",
   "metadata": {},
   "source": [
    "### 64 units per layer with 0.3 dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "59a51791-de89-40a8-9f9b-74f7e598bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBModel(nn.Module):\n",
    "    def __init__(self, input_size): \n",
    "        super(IMDBModel, self).__init__() \n",
    "        self.dense1 = nn.Linear(input_size, 64) \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(64, 64) \n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        val = x\n",
    "        for _map in [self.dense1,\n",
    "                     self.activation , \n",
    "                     self.dense2, \n",
    "                     self.activation , \n",
    "                     self.output,\n",
    "                     self.dropout]:\n",
    "            val = _map(val) \n",
    "            return torch.flatten(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "43bdb8f2-c36a-4811-b6c9-854bb907fc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "IMDBModel                                [25000, 10003]            [1600000]                 4,225\n",
       "├─Linear: 1-1                            [25000, 10003]            [25000, 64]               640,256\n",
       "===================================================================================================================\n",
       "Total params: 644,481\n",
       "Trainable params: 644,481\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 16.01\n",
       "===================================================================================================================\n",
       "Input size (MB): 1000.30\n",
       "Forward/backward pass size (MB): 12.80\n",
       "Params size (MB): 2.56\n",
       "Estimated Total Size (MB): 1015.66\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_model = IMDBModel(imdb_test.tensors[0].size()[1]) \n",
    "summary(imdb_model,\n",
    "        input_size=imdb_test.tensors[0].size(), \n",
    "        col_names=['input_size','output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e984cd0-a5a5-488c-8a2f-31168bc0c9fd",
   "metadata": {},
   "source": [
    "Having loaded the datasets into a data module and created a SimpleModule, the remaining steps are familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bba357-a0cd-4cf1-b50f-fd4c7dd0935f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
