{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f552ba-b214-4435-b7dc-d23a8af497f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e71813-66cb-4ab9-b883-3fc5447d20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state: 4 elements: car position, cart velocity, pole angle, pole angular velocity\n",
    "print(env.observation_space.shape)\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18600b-d53d-45b2-be3a-29adcd8e50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The episode terminates if the pole angle is not in the range (-.2095, .2095)\n",
    "\n",
    "# random action\n",
    "for episode in range(10):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    for t in range(100):\n",
    "    # while not terminated:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        score += reward\n",
    "        print('step {}, action {}, reward {}, state {}'.format(t,action,reward*(not done), state))\n",
    "        print(f\"Episode {episode} score {score}\")\n",
    "       # if done:\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "944a0e93-b7b4-4eb1-9bea-b0b87b908d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent class that will be used in Q-learning\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, policy='random'):\n",
    "        self.total_reward = 0\n",
    "        self.policy = policy\n",
    "        self.alpha = 0.1  # learning rate\n",
    "        self.epsilon = 1  # epsilon-greedy\n",
    "        self.gamma = 1 # discount factor, DON'T CHANGE THIS VALUE FOR HOMEWORK problem 2. \n",
    "        self.theta = np.zeros([2,1,4,3,2])\n",
    "        \n",
    "    def obs_index(self, state):\n",
    "        bins = (np.array([0]),\n",
    "                np.array([1e20]),\n",
    "                np.array([-0.2, 0, 0.2]),\n",
    "                np.array([-0.3, 0.3])\n",
    "               )\n",
    "        ind=np.zeros(4).astype(int)\n",
    "        for i in range(len(state)):\n",
    "            ind[i] = np.digitize(state[i],bins[i])\n",
    "        return tuple(ind)\n",
    "    \n",
    "    def q(self, state):\n",
    "        ind = self.obs_index(state)\n",
    "        return self.theta[ind]\n",
    "    \n",
    "    def q_update(self, last_state, action, reward, state):\n",
    "        ind = self.obs_index(state)\n",
    "        ind_last = self.obs_index(last_state)\n",
    "        delta = (reward + self.gamma*np.max(self.theta[ind]) - self.theta[ind_last+(action,)])\n",
    "        self.theta[ind_last+(action,)] += self.alpha*delta\n",
    "\n",
    "    def choose_action(self, state, episode):\n",
    "        if self.policy=='random':\n",
    "            return int(np.round(np.random.random()))\n",
    "        elif self.policy=='eps_greedy':\n",
    "            if np.random.rand()>self.epsilon:\n",
    "                if self.q(state)[0]>self.q(state)[1]:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return int(np.round(np.random.random()))\n",
    "\n",
    "    def gather_reward(self, reward, t):\n",
    "        self.total_reward += (self.gamma**t)*reward\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "    def set_total_reward(self, new_total):\n",
    "        self.total_reward = new_total\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.theta[1,0, 0,0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65ea10-e174-4b4b-8088-f7d2f03899b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training q-learning with epsilon-greedy action\n",
    "policy = 'eps_greedy'\n",
    "agent = Agent(policy)\n",
    "ep_rewards = []\n",
    "\n",
    "epi_length = 500 # number of episodes in training\n",
    "maxT = 51        # maximal number of steps in each episode\n",
    "for episode in range(epi_length):\n",
    "    last_state, info = env.reset()\n",
    "    agent.set_total_reward(0)\n",
    "    done = False\n",
    "    for t in range(maxT):\n",
    "        action = agent.choose_action(last_state, episode)\n",
    "        state,reward,done,truncated,info=env.step(action)\n",
    "        agent.gather_reward(reward,t)\n",
    "        agent.q_update(last_state,action,reward,state)\n",
    "        last_state=state\n",
    "        # env.render()\n",
    "        if done==True or t==maxT-1:\n",
    "            ep_rewards.append(agent.get_total_reward())\n",
    "            print(episode, t, agent.get_total_reward())\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal Q-function learned from Q-learning\n",
    "opt_theta = agent.theta\n",
    "opt_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73877d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal policy learned from Q-learning\n",
    "def opt_action(theta, state):\n",
    "    ind = agent.obs_index(state)\n",
    "    if theta[ind][0]> theta[ind][1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c787d-489a-432e-b51f-bf478118c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run estimated optimal policy on CartPole for 50 episodes and a maximal 201 steps in each episode\n",
    "\n",
    "ep_rewards = []\n",
    "for episode in range(50):\n",
    "    last_state, info = env.reset()\n",
    "    agent.set_total_reward(0)\n",
    "    done = False\n",
    "    for t in range(201):\n",
    "        action = opt_action(opt_theta, last_state)\n",
    "        state,reward,done,truncated,info=env.step(action)\n",
    "        agent.gather_reward(reward,t)\n",
    "        last_state=state\n",
    "           # print(t+1, agent.get_total_reward(), done, episode, state)\n",
    "        env.render()\n",
    "        if done==True or t==200:\n",
    "            ep_rewards.append(agent.get_total_reward())\n",
    "            print(episode, t, agent.get_total_reward())\n",
    "            break\n",
    "       \n",
    "avg_reward = np.round(np.mean(ep_rewards),1)\n",
    "sd_reward = np.round(np.std(ep_rewards),1)\n",
    "plt.plot(ep_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Performance of Estimated Optimal Policy on CartPole')\n",
    "plt.figtext(0.5, -0.1, f\"Total return per episode (mean ± sd over 50 episodes): {avg_reward} ± {sd_reward}\", wrap=True, horizontalalignment='center', fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
