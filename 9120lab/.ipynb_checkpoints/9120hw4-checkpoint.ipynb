{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54585b7-3863-4c96-a41b-811d24d051f9",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824e478-16ce-4d97-8ca8-2f097d38c4df",
   "metadata": {},
   "source": [
    "### (a) Suppose Rs = +1. Consider an equiprobable random policy $\\pi$ (i.e., all actions equally likely). Implement policy evaluation algorithm to compute $V \\pi$. Report the value function on a grid, similar to the example shown on slide 26 of lecture 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7afc47-1e5b-41b8-949f-506351ae206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37f81270-539b-4101-817e-76ff357c67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "gamma = 1  # discount factor\n",
    "Ry = -5    # reward for yellow death state (state 0)\n",
    "Rb = +5    # reward for blue target state (state 15)\n",
    "Rs = 1     # reward for states 1-14\n",
    "\n",
    "# states and actions\n",
    "n_states = 16\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "# rewards\n",
    "rewards = np.ones(n_states) * Rs\n",
    "rewards[0] = Ry\n",
    "rewards[15] = Rb\n",
    "\n",
    "# transition function\n",
    "def get_next_state(state, action):\n",
    "    if state == 0 or state == 15:\n",
    "        return state  # final states\n",
    "    grid = np.arange(n_states).reshape(4, 4)\n",
    "    i, j = np.where(grid == state)\n",
    "    i, j = i[0], j[0] # i row, j col\n",
    "    \n",
    "    if action == \"up\" and i > 0:\n",
    "        i -= 1 # row - 1 \n",
    "    elif action == \"down\" and i < 3:\n",
    "        i += 1 # row + 1 \n",
    "    elif action == \"left\" and j > 0:\n",
    "        j -= 1 # col - 1\n",
    "    elif action == \"right\" and j < 3:\n",
    "        j += 1 # col + 1 \n",
    "    return grid[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a56cfe6-6eea-4a4e-98d9-e2773b2dbdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.25 0.25 0.25 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.25 0.25 0.25 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.25 0.5  0.   0.   0.   0.25 0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.   0.   0.25 0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.25 0.   0.   0.25 0.   0.25 0.   0.   0.25 0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.25 0.   0.   0.25 0.   0.25 0.   0.   0.25 0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.25 0.   0.   0.25 0.25 0.   0.   0.   0.25 0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.25 0.   0.   0.   0.25 0.25 0.   0.   0.25 0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.   0.25 0.   0.   0.25\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.   0.25 0.   0.\n",
      "  0.25 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.25 0.   0.\n",
      "  0.   0.25]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.   0.5  0.25\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.25\n",
      "  0.25 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.25\n",
      "  0.25 0.25]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# random policy\n",
    "transition_matrix = np.zeros((n_states, n_states))\n",
    "for s in range(n_states):\n",
    "    for a in actions:\n",
    "        next_state = get_next_state(s, a)\n",
    "        transition_matrix[s, next_state] += 1 / len(actions)\n",
    "\n",
    "print(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "116b599f-0cb0-4929-94ad-a459ad6e0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy evaluation function\n",
    "def policy_evaluation(transition_matrix, rewards, gamma, threshold=1e-6):\n",
    "    V = np.zeros(n_states)  # initialize value function\n",
    "    delta = float(\"inf\")\n",
    "    while delta > threshold:\n",
    "        delta = 0\n",
    "        V_new = np.zeros_like(V)\n",
    "        for s in range(n_states):\n",
    "            V_new[s] = sum(\n",
    "                transition_matrix[s, sp] * (rewards[sp] + gamma * V[sp])\n",
    "                for sp in range(n_states)\n",
    "            )\n",
    "        delta = np.max(np.abs(V_new - V))\n",
    "        V = V_new\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fba4e6fb-d4f9-4cf4-b13a-53d1b5193e30",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Vpi\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m V_pi \u001b[38;5;241m=\u001b[39m policy_evaluation(transition_matrix, rewards, gamma)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(V_pi\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m, in \u001b[0;36mpolicy_evaluation\u001b[0;34m(transition_matrix, rewards, gamma, threshold)\u001b[0m\n\u001b[1;32m      7\u001b[0m V_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(V)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_states):\n\u001b[0;32m----> 9\u001b[0m     V_new[s] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m     10\u001b[0m         transition_matrix[s, sp] \u001b[38;5;241m*\u001b[39m (rewards[sp] \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m V[sp])\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_states)\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(V_new \u001b[38;5;241m-\u001b[39m V))\n\u001b[1;32m     14\u001b[0m V \u001b[38;5;241m=\u001b[39m V_new\n",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m V_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(V)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_states):\n\u001b[1;32m      9\u001b[0m     V_new[s] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m---> 10\u001b[0m         transition_matrix[s, sp] \u001b[38;5;241m*\u001b[39m (rewards[sp] \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m V[sp])\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_states)\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(V_new \u001b[38;5;241m-\u001b[39m V))\n\u001b[1;32m     14\u001b[0m V \u001b[38;5;241m=\u001b[39m V_new\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Vpi\n",
    "V_pi = policy_evaluation(transition_matrix, rewards, gamma)\n",
    "print(V_pi.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "981c661f-7d28-48f8-a705-858a89e9013b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c8eae9f-0bce-45af-97d4-d2555030d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e7eee20-f1fa-42ca-8a1a-2a85c7b03986",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1 # discounting rate\n",
    "gridSize = 4\n",
    "terminationStates = [[0,0], [gridSize-1, gridSize-1]]\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]\n",
    "numIterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e76189e6-1405-4699-ad15-70ae1ad36f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def actionRewardFunction(initialPosition, action):\n",
    "    # Check if the state is a termination state\n",
    "    if initialPosition in terminationStates:\n",
    "        return initialPosition, 0\n",
    "    \n",
    "    # Define the reward map for specific states\n",
    "    rewardMap = {\n",
    "        (0, 0): -5,  # State 0\n",
    "        (3, 3): 5    # State 15\n",
    "    }\n",
    "    defaultReward = 1  # Default reward for all other states\n",
    "\n",
    "    # Calculate the next position after taking the action\n",
    "    finalPosition = np.array(initialPosition) + np.array(action)\n",
    "    \n",
    "    # Check for out-of-bounds moves and reset position if needed\n",
    "    if -1 in finalPosition or gridSize in finalPosition:\n",
    "        finalPosition = np.array(initialPosition)  # Invalid move; stay in the same position\n",
    "    \n",
    "    # Convert finalPosition to a tuple to check in the rewardMap\n",
    "    reward = rewardMap.get(tuple(finalPosition), defaultReward)\n",
    "    return finalPosition.tolist(), reward\n",
    "\n",
    "\n",
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "# values of the value function at step 0\n",
    "valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "865ed4f6-1cf4-4363-8492-ab30b44ba60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "Terminal state neighbors at iteration 1:\n",
      "State [0, 1] value: 10.692307692307676\n",
      "State [1, 0] value: 10.692307692307677\n",
      "State [3, 2] value: 15.307692307692292\n",
      "State [2, 3] value: 15.307692307692289\n",
      "\n",
      "Iteration 2\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "Terminal state neighbors at iteration 2:\n",
      "State [0, 1] value: 10.692307692307676\n",
      "State [1, 0] value: 10.692307692307677\n",
      "State [3, 2] value: 15.307692307692292\n",
      "State [2, 3] value: 15.307692307692289\n",
      "\n",
      "Iteration 3\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "Terminal state neighbors at iteration 3:\n",
      "State [0, 1] value: 10.692307692307676\n",
      "State [1, 0] value: 10.692307692307677\n",
      "State [3, 2] value: 15.307692307692292\n",
      "State [2, 3] value: 15.307692307692289\n",
      "\n",
      "Iteration 10\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "Terminal state neighbors at iteration 10:\n",
      "State [0, 1] value: 10.692307692307676\n",
      "State [1, 0] value: 10.692307692307677\n",
      "State [3, 2] value: 15.307692307692292\n",
      "State [2, 3] value: 15.307692307692289\n",
      "\n",
      "Iteration 100\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "Terminal state neighbors at iteration 100:\n",
      "State [0, 1] value: 10.692307692307676\n",
      "State [1, 0] value: 10.692307692307677\n",
      "State [3, 2] value: 15.307692307692292\n",
      "State [2, 3] value: 15.307692307692289\n",
      "\n",
      "Iteration 1000\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "Terminal state neighbors at iteration 1000:\n",
      "State [0, 1] value: 10.692307692307676\n",
      "State [1, 0] value: 10.692307692307677\n",
      "State [3, 2] value: 15.307692307692292\n",
      "State [2, 3] value: 15.307692307692289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check neighbors of terminal states\n",
    "terminal_neighbors = [\n",
    "    [0, 1], [1, 0],   # States next to [0, 0]\n",
    "    [3, 2], [2, 3]    # States next to [3, 3]\n",
    "]\n",
    "\n",
    "for it in range(numIterations):\n",
    "    copyValueMap = np.copy(valueMap)  # Create a copy to store updated values\n",
    "    deltaState = []  # Track maximum delta for convergence monitoring\n",
    "\n",
    "    for state in states:\n",
    "        weightedRewards = 0\n",
    "        for action in actions:\n",
    "            finalPosition, reward = actionRewardFunction(state, action)\n",
    "            weightedRewards += (1 / len(actions)) * (reward + (gamma * valueMap[finalPosition[0], finalPosition[1]]))\n",
    "        \n",
    "        deltaState.append(np.abs(copyValueMap[state[0], state[1]] - weightedRewards))\n",
    "        copyValueMap[state[0], state[1]] = weightedRewards\n",
    "    \n",
    "    deltas.append(max(deltaState))  # Track the maximum delta for each iteration\n",
    "    valueMap = copyValueMap  # Update the value map with the newly computed values\n",
    "    \n",
    "    # Output the value map at specific iterations for debugging\n",
    "    if it in [0, 1, 2, 9, 99, numIterations - 1]:\n",
    "        print(f\"Iteration {it + 1}\")\n",
    "        print(valueMap)\n",
    "        \n",
    "        # Print the values of terminal state neighbors\n",
    "        print(f\"Terminal state neighbors at iteration {it + 1}:\")\n",
    "        for neighbor in terminal_neighbors:\n",
    "            print(f\"State {neighbor} value: {valueMap[neighbor[0], neighbor[1]]}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca8f3680-0265-4dbe-88dc-b2b5ccacdd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "\n",
      "Iteration 2\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "\n",
      "Iteration 3\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "\n",
      "Iteration 10\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "\n",
      "Iteration 100\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "\n",
      "Iteration 1000\n",
      "[[ 0.         10.69230769 18.23076923 21.        ]\n",
      " [10.69230769 15.84615385 19.         19.76923077]\n",
      " [18.23076923 19.         18.15384615 15.30769231]\n",
      " [21.         19.76923077 15.30769231  0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## policy evaluation\n",
    "deltas = []\n",
    "for it in range(numIterations):\n",
    "    copyValueMap = np.copy(valueMap)\n",
    "    deltaState = []\n",
    "    for state in states:\n",
    "        weightedRewards = 0\n",
    "        for action in actions:\n",
    "            finalPosition, reward = actionRewardFunction(state, action)\n",
    "            weightedRewards += (1/len(actions))*(reward+(gamma*valueMap[finalPosition[0], finalPosition[1]]))\n",
    "        deltaState.append(np.abs(copyValueMap[state[0], state[1]]-weightedRewards))\n",
    "        copyValueMap[state[0], state[1]] = weightedRewards\n",
    "    deltas.append(deltaState)\n",
    "    valueMap = copyValueMap\n",
    "    if it in [0,1,2,9, 99, numIterations-1]:\n",
    "        print(\"Iteration {}\".format(it+1))\n",
    "        print(valueMap)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8116c8-f780-4950-9cb3-6a95a4fd3d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
