{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2  Using Regression/Tree-based/SVM/NN models for classification in python\n",
    "\n",
    "In this lab, we will implement a series of machine learning methods for classification using scikit-learn package. We will use the built-in cross validation for each model to select the corresponding hyper-parameters, and we can compare their performances using Accuracy/AUC/AUPRC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn\n",
    "\n",
    "Scikit-learn is a powerful and easy-to-use Python library for machine learning. It provides simple and efficient tools for data mining and data analysis, making it accessible to both beginners and experienced practitioners. The library is built on top of NumPy, SciPy, and matplotlib, ensuring seamless integration with these popular scientific computing libraries.\n",
    "\n",
    "## Key Features of scikit-learn:\n",
    "- **Classification**\n",
    "- **Regression**\n",
    "- **Clustering**\n",
    "- **Dimensionality Reduction**\n",
    "- **Model Selection**\n",
    "- **Preprocessing**\n",
    "\n",
    "Scikit-learn's consistent API, comprehensive documentation, and active community make it a go-to library for machine learning tasks in Python.\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a simple example where we fit a RandomForestClassifier to some very basic data:\n",
    "# Sample data: 2 samples with 3 features each\n",
    "X = [[1, 2, 3],  \n",
    "    [11, 12, 13]]\n",
    "\n",
    "# Classes of each sample\n",
    "y = [0, 1]\n",
    "\n",
    "# Import the LogisticRegression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "# Initialize the RandomForestClassifier with a fixed random state for reproducibility\n",
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "# Fit the model to the data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict  classes of the training data\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes\n",
    "print(clf.classes_)\n",
    "print(clf.coef_)\n",
    "print(clf.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data\n",
    "\n",
    "Generate a more complicated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import matplotlib.pyplot as plt # Plotting library\n",
    "import numpy as np  # Numerical library\n",
    "import pandas as pd # Dataframe library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "import sklearn.datasets\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.1) # 200 samples, the larger the noise, the less clear the moons are\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting and evaluating machine learning models:\n",
    "\n",
    "**General steps:**\n",
    "1. Split data into training/testing sets\n",
    "    * Data standardization\n",
    "2. Fit the model using training set\n",
    "    * Apply cross-validation strategies to select hyper-parameters\n",
    "    * Re-fit the model using the best hyper-parameters\n",
    "3. Evaluate the model using testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data with shuffle and stratify\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,  # 30% of the data will be used for testing\n",
    "    random_state=42,  # Ensures reproducibility\n",
    "    shuffle=True,  # Shuffle the data before splitting to ensure the data is well-distributed\n",
    "    stratify=y  # Preserve the class distribution in both sets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(X_train.mean(axis=0))\n",
    "print(X_test.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# StandardScaler is to standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.mean(axis=0))\n",
    "print(X_test.mean(axis=0))\n",
    "\n",
    "print(X_train.std(axis=0))\n",
    "print(X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation in scikit-learn\n",
    "\n",
    "Most of the machine learning models have hyper-parameters. We need to tune them based on our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hyperparameters of a model\n",
    "clf.get_params() # Here is the list of hyperparameters of the LogisticRegression we specified before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a general function in scikit-learn for cv: `sklearn.model_selection.GridSearchCV`. \n",
    "\n",
    "**General Steps to Use `GridSearchCV`**\n",
    "\n",
    "1. **Setup Parameters:**\n",
    "    - Define the parameter grid that you want to search over. \n",
    "    ```\n",
    "    param_grid = {\n",
    "         'param1': [value1, value2, ...],\n",
    "         'param2': [value1, value2, ...],\n",
    "         ...\n",
    "    }\n",
    "    ```\n",
    "2. **Setup Model:**\n",
    "    - Initialize the machine learning model you want to tune.\n",
    "     ```\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.some_model import SomeModel\n",
    "\n",
    "    model = SomeModel()\n",
    "    ```\n",
    "3. **Initialize `GridSearchCV`**\n",
    "     * Set the model, parameter grid, and other important parameters such as cross-validation strategy and scoring metric.\n",
    "     ```\n",
    "     grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "     ```\n",
    "4. **Fit the `GridSearchCV`**\n",
    "     * Fit using the training data to perform the grid search\n",
    "     ```\n",
    "     grid_search.fit(X_train, y_train)\n",
    "     ```\n",
    "5. **Retrieve the best model and its parameters.**\n",
    "     * Retrive the best hyper-parameter: \n",
    "     ```\n",
    "     print('Best hyperparameters:', grid_search.best_params_)\n",
    "     ```\n",
    "     * Fit the model with the best hyperpar using all training samples \n",
    "    ```\n",
    "    best_model = grid_search.best_estimator_\n",
    "    ```\n",
    "6. **Evaluate the model using the test data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check more detailed GridSearchCV examples at: https://scikit-learn.org/stable/modules/cross_validation.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "We will use 3 metrics to evaluate the model performance:\n",
    "* Accuracy: The ratio of correctly predicted instances to the total instances.\n",
    "* AUROC: The Area Under the Receiver Operating Characteristic curve, which measures the ability of the model to distinguish between classes.\n",
    "* AUPRC: The Area Under the Precision-Recall Curve, which evaluates the trade-off between precision and recall for different threshold settings.\n",
    "\n",
    "**Using `sklearn.metrics`:**\n",
    "```\n",
    "from sklearn import metrics\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate AUROC\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"AUROC: {auc}\")\n",
    "\n",
    "# Calculate AUPRC\n",
    "auprc = metrics.average_precision_score(y_test, y_pred_prob)\n",
    "print(f\"AUPRC: {auprc}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to implement some models we learned in class\n",
    "# We first set an empty list store the performance of the different models\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with L1 regularization, use cv to find the best hyperparameter\n",
    "from sklearn.linear_model import LogisticRegression # Logistic Regression in sklearn\n",
    "from sklearn import metrics # metrics to evaluate the model\n",
    "from sklearn.model_selection import GridSearchCV # Grid search to find the best hyperparameter\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "params = {\"C\": np.logspace(-3,3,7)} # Inverse of regularization strength\n",
    "\n",
    "# Initialize the model          \n",
    "model = LogisticRegression(penalty='l2', solver='saga', tol=1e-8) \n",
    "\n",
    "# Initialize the GridSearchCV with 5-fold cross-validation\n",
    "grid = GridSearchCV(model, param_grid=params, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "print('Best hyperparameter:', grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Get the predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_prob = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Get the performance\n",
    "test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "test_auc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "test_auprc = metrics.average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "# Store the results\n",
    "results.append({\"model\": 'Logistic regression with L1 penalty', \"test_accuracy\": test_accuracy, \"test_auc\": test_auc, \"test_auprc\": test_auprc})\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: compare performance of different hyperparameters\n",
    "plt.figure()\n",
    "plt.plot(grid.cv_results_[\"param_C\"], grid.cv_results_[\"mean_test_score\"])\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Mean test auc\")\n",
    "plt.title(\"Performance of different hyperparameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree for classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# ?DecisionTreeClassifier\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "params = {\"max_depth\": np.arange(1, 11)}\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "\n",
    "# Initialize the GridSearchCV with 5-fold cross-validation\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "\n",
    "\n",
    "# Get the best model\n",
    "\n",
    "\n",
    "# Get the predictions\n",
    "\n",
    "\n",
    "# Get the performance\n",
    "\n",
    "\n",
    "# Store the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest for classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# ?RandomForestClassifier\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "params = {\"n_estimators\": [100, 200, 300, 400, 500]} \n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "\n",
    "# Initialize the GridSearchCV with 5-fold cross-validation\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "\n",
    "\n",
    "# Get the best model\n",
    "\n",
    "\n",
    "# Get the predictions\n",
    "\n",
    "\n",
    "# Get the performance\n",
    "\n",
    "\n",
    "# Store the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting for classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# ?GradientBoostingClassifier\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "params = {\"n_estimators\": [100, 200, 300, 400, 500], \"learning_rate\": np.logspace(-3,0,4)} \n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "\n",
    "# Initialize the GridSearchCV with 5-fold cross-validation\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "\n",
    "\n",
    "# Get the best model\n",
    "\n",
    "\n",
    "# Get the predictions\n",
    "\n",
    "\n",
    "# Get the performance\n",
    "\n",
    "\n",
    "# Store the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine for classification\n",
    "from sklearn.svm import SVC\n",
    "# ?SVC\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "params = {\"C\": np.logspace(-3,3,7)} \n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "\n",
    "# Initialize the GridSearchCV with 5-fold cross-validation\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "\n",
    "\n",
    "# Get the best model\n",
    "\n",
    "\n",
    "# Get the predictions\n",
    "\n",
    "\n",
    "# Get the performance\n",
    "\n",
    "\n",
    "# Store the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network for classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# ?MLPClassifier\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "params = {\"hidden_layer_sizes\": [(4,), (8,)],  # number of neurons in the hidden layer\n",
    "          \"alpha\": np.logspace(-3,0,4), # L2 regularization\n",
    "          'learning_rate_init': np.logspace(-3,0,4) # initial learning rate\n",
    "          } \n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "\n",
    "# Initialize the GridSearchCV with 5-fold cross-validation\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "\n",
    "\n",
    "# Get the best model\n",
    "\n",
    "\n",
    "# Get the predictions\n",
    "\n",
    "\n",
    "# Get the performance\n",
    "\n",
    "\n",
    "# Store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
